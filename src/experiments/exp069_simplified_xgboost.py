"""
EXPERIMENT: EXP-069
Date: 2025-11-17
Objective: Retrain XGBoost with 41 production-compatible features (no SPY dependencies)

HYPOTHESIS:
XGBoost model trained on 41 technical features (excluding SPY-relative metrics)
can still achieve strong predictive performance (AUC > 0.70) while being
compatible with real-time production scanner.

ALGORITHM TESTED:
XGBoost Gradient Boosting Classifier
- Input: 41 technical features (no market-relative features)
- Target: Binary (will have profitable panic sell + recovery in next 7 days)
- Validation: Walk-forward time-series split (5-fold)
- Comparison: Compare to EXP-067 (48 features, 0.843 AUC)

EXPECTED IMPACT:
- Maintain >= 0.70 AUC (acceptable tradeoff for production compatibility)
- Enable immediate production deployment
- Eliminate SPY data dependency
- Simplify feature engineering pipeline

SUCCESS CRITERIA:
- Out-of-sample AUC >= 0.70
- Precision >= 0.25
- Ready for production scanner integration
"""

import sys
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# ML imports
try:
    import xgboost as xgb
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import (
        roc_auc_score, precision_score, recall_score,
        classification_report, confusion_matrix
    )
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("[WARNING] ML libraries not installed")
    print("Install: pip install xgboost scikit-learn")


def load_feature_matrix(feature_file: str) -> pd.DataFrame:
    """Load the feature matrix generated by EXP-066."""
    try:
        df = pd.read_csv(feature_file)
        print(f"[OK] Loaded feature matrix: {len(df)} rows, {len(df.columns)} columns")
        return df
    except Exception as e:
        print(f"[ERROR] Failed to load feature matrix: {e}")
        return None


def prepare_simplified_ml_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, List[str]]:
    """
    Prepare data for ML training using only 41 production-compatible features.

    Excludes SPY-relative features:
    - spy_close
    - spy_return_1d
    - spy_return_20d
    - rel_perf_1d
    - rel_perf_20d
    - spy_corr_60
    - beta_60

    Returns:
        X: Feature matrix (41 features)
        y: Target variable
        feature_names: List of feature column names
    """
    # 41 production-compatible features (same as scanner)
    production_features = [
        'return_1d', 'return_5d', 'return_10d', 'return_20d', 'return_60d',
        'log_return_1d', 'log_return_5d', 'rsi_14', 'rsi_28',
        'macd', 'macd_signal', 'macd_hist', 'stoch_k', 'stoch_d',
        'roc_10', 'roc_20', 'bb_upper', 'bb_middle', 'bb_lower',
        'bb_width', 'bb_position', 'atr_14', 'volatility_10',
        'volatility_20', 'volatility_60', 'volume_sma_20', 'volume_ratio',
        'obv', 'obv_sma_20', 'vwap_20', 'sma_10', 'sma_20', 'sma_50',
        'sma_200', 'ema_10', 'ema_20', 'ema_50', 'sma_10_20_ratio',
        'sma_20_50_ratio', 'price_sma_50_ratio', 'price_sma_200_ratio'
    ]

    # Verify all features exist
    missing_features = [f for f in production_features if f not in df.columns]
    if missing_features:
        print(f"[ERROR] Missing features: {missing_features}")
        return None, None, None

    # Handle missing values
    df_clean = df[production_features + ['target']].copy()
    df_clean = df_clean.fillna(method='ffill').fillna(method='bfill')
    df_clean = df_clean.dropna()

    X = df_clean[production_features]
    y = df_clean['target']

    print(f"[OK] Prepared simplified ML data: {len(X)} samples, {len(production_features)} features")
    print(f"    Features excluded: 7 SPY-relative features (spy_close, spy_return_*, rel_perf_*, beta_60, spy_corr_60)")
    print(f"    Target distribution: {y.sum()} positive ({y.mean()*100:.1f}%), "
          f"{len(y) - y.sum()} negative ({(1-y.mean())*100:.1f}%)")

    return X, y, production_features


def train_simplified_xgboost(X: pd.DataFrame, y: pd.Series,
                             feature_names: List[str]) -> Dict:
    """
    Train simplified XGBoost model with walk-forward validation.
    """
    print("\n" + "="*70)
    print("TRAINING SIMPLIFIED XGBOOST MODEL (41 FEATURES)")
    print("="*70)

    # Time-series split for walk-forward validation
    n_splits = 5
    tscv = TimeSeriesSplit(n_splits=n_splits)

    # Store results from each fold
    fold_results = []
    feature_importance_all = []

    print(f"\nUsing {n_splits}-fold time-series cross-validation...")
    print("-" * 70)

    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
        print(f"\nFold {fold}/{n_splits}")

        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        print(f"  Train: {len(X_train)} samples ({y_train.sum()} positive)")
        print(f"  Test:  {len(X_test)} samples ({y_test.sum()} positive)")

        # Calculate scale_pos_weight for class imbalance
        scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()

        # Train XGBoost
        model = xgb.XGBClassifier(
            objective='binary:logistic',
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=scale_pos_weight,
            random_state=42,
            n_jobs=-1
        )

        model.fit(X_train, y_train, verbose=False)

        # Predictions
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        y_pred = model.predict(X_test)

        # Metrics
        auc = roc_auc_score(y_test, y_pred_proba)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)

        print(f"  AUC: {auc:.3f} | Precision: {precision:.3f} | Recall: {recall:.3f}")

        fold_results.append({
            'fold': fold,
            'auc': float(auc),
            'precision': float(precision),
            'recall': float(recall),
            'test_samples': len(X_test),
            'test_positives': int(y_test.sum())
        })

        # Feature importance
        feature_importance_all.append(model.feature_importances_)

    # Aggregate results
    avg_auc = np.mean([r['auc'] for r in fold_results])
    avg_precision = np.mean([r['precision'] for r in fold_results])
    avg_recall = np.mean([r['recall'] for r in fold_results])

    print("\n" + "="*70)
    print("CROSS-VALIDATION RESULTS (SIMPLIFIED MODEL)")
    print("="*70)
    print(f"Average AUC:       {avg_auc:.3f}")
    print(f"Average Precision: {avg_precision:.3f}")
    print(f"Average Recall:    {avg_recall:.3f}")

    # Feature importance (average across folds)
    avg_importance = np.mean(feature_importance_all, axis=0)
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': avg_importance
    }).sort_values('importance', ascending=False)

    print("\nTop 15 Most Important Features:")
    print("-" * 70)
    for idx, row in feature_importance_df.head(15).iterrows():
        print(f"  {row['feature']:30s} | {row['importance']:.4f}")

    # Train final model on all data
    print("\nTraining final model on full dataset...")
    scale_pos_weight_final = (len(y) - y.sum()) / y.sum()

    final_model = xgb.XGBClassifier(
        objective='binary:logistic',
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight_final,
        random_state=42,
        n_jobs=-1
    )

    final_model.fit(X, y, verbose=False)

    return {
        'avg_auc': avg_auc,
        'avg_precision': avg_precision,
        'avg_recall': avg_recall,
        'fold_results': fold_results,
        'feature_importance': feature_importance_df.to_dict('records'),
        'model': final_model
    }


def run_exp069_simplified_xgboost():
    """
    Train and evaluate simplified XGBoost model (41 features).
    """
    print("="*70)
    print("EXP-069: SIMPLIFIED XGBOOST MODEL (PRODUCTION-COMPATIBLE)")
    print("="*70)
    print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    print("OBJECTIVE: Retrain XGBoost with 41 production-compatible features")
    print("ALGORITHM: XGBoost Gradient Boosting Classifier")
    print("FEATURES: 41 technical indicators (NO SPY-relative features)")
    print("TARGET: Panic sell (>3% drop) + 7-day recovery (>3% gain)")
    print()

    if not ML_AVAILABLE:
        print("[ERROR] XGBoost/sklearn not available")
        print("Install: pip install xgboost scikit-learn")
        return None

    # Load feature matrix from EXP-066
    feature_file = 'logs/experiments/exp066_feature_matrix.csv'

    if not os.path.exists(feature_file):
        print(f"[ERROR] Feature matrix not found: {feature_file}")
        print("Run EXP-066 first to generate features")
        return None

    df = load_feature_matrix(feature_file)
    if df is None:
        return None

    print(f"\nDataset: {len(df)} total samples")
    print(f"Date range: {df['Date'].min()} to {df['Date'].max()}")
    print(f"Stocks: {df['ticker'].nunique()} unique tickers")
    print()

    # Prepare data with simplified features
    X, y, feature_names = prepare_simplified_ml_data(df)
    if X is None:
        return None

    # Train model
    results = train_simplified_xgboost(X, y, feature_names)

    # Compare to EXP-067
    exp067_auc = 0.843
    exp067_precision = 0.369
    performance_ratio = results['avg_auc'] / exp067_auc

    print("\n" + "="*70)
    print("COMPARISON TO EXP-067 (48 FEATURES)")
    print("="*70)
    print(f"EXP-067 (48 features): AUC {exp067_auc:.3f}, Precision {exp067_precision:.3f}")
    print(f"EXP-069 (41 features): AUC {results['avg_auc']:.3f}, Precision {results['avg_precision']:.3f}")
    print(f"Performance retained: {performance_ratio*100:.1f}%")
    print()

    # Deployment decision
    deploy = results['avg_auc'] >= 0.70 and results['avg_precision'] >= 0.25

    print("="*70)
    print("DEPLOYMENT DECISION")
    print("="*70)
    print()

    if deploy:
        print(f"[SUCCESS] Simplified XGBoost model ready for production!")
        print()
        print(f"[OK] AUC: {results['avg_auc']:.3f} (>= 0.70 required)")
        print(f"[OK] Precision: {results['avg_precision']:.3f} (>= 0.25 required)")
        print()
        print("BENEFITS:")
        print(f"  - Production-compatible (41 features, no SPY dependency)")
        print(f"  - {performance_ratio*100:.1f}% of full model performance")
        print(f"  - Precision: {results['avg_precision']*100:.1f}% (vs 15% baseline = {results['avg_precision']/0.15:.1f}x improvement)")
        print(f"  - Recall: {results['avg_recall']*100:.1f}% opportunity coverage")
        print()
        print("NEXT STEP: Update EXP-068 scanner to use simplified model")
        print("  - Replace exp067_xgboost_model.json with exp069_simplified_model.json")
        print("  - Run end-to-end production test")
        print("  - Deploy to daily scanner")
    else:
        reasons = []
        if results['avg_auc'] < 0.70:
            reasons.append(f"AUC {results['avg_auc']:.3f} < 0.70 required")
        if results['avg_precision'] < 0.25:
            reasons.append(f"Precision {results['avg_precision']:.3f} < 0.25 required")

        print("[INSUFFICIENT PERFORMANCE]")
        for reason in reasons:
            print(f"  - {reason}")
        print()
        print("OPTIONS:")
        print("  A. Accept lower performance for production compatibility")
        print("  B. Add SPY data fetching to production scanner (Path B from EXP-068)")
        print("  C. Try different hyperparameters or feature engineering")

    # Save results
    results_dir = 'logs/experiments'
    os.makedirs(results_dir, exist_ok=True)

    exp_results = {
        'experiment_id': 'EXP-069',
        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'objective': 'Retrain XGBoost with 41 production-compatible features (no SPY dependencies)',
        'algorithm': 'XGBoost Gradient Boosting Classifier (5-fold time-series CV)',
        'dataset': {
            'samples': len(df),
            'features': len(feature_names),
            'date_range': f"{df['Date'].min()} to {df['Date'].max()}",
            'stocks': int(df['ticker'].nunique())
        },
        'performance': {
            'avg_auc': float(results['avg_auc']),
            'avg_precision': float(results['avg_precision']),
            'avg_recall': float(results['avg_recall'])
        },
        'comparison': {
            'exp067_auc': exp067_auc,
            'exp067_precision': exp067_precision,
            'performance_retained_pct': float(performance_ratio * 100)
        },
        'fold_results': results['fold_results'],
        'top_features': results['feature_importance'][:20],
        'deploy': deploy,
        'next_step': 'Update EXP-068 scanner' if deploy else 'Evaluate tradeoffs',
        'hypothesis': 'Simplified model maintains strong performance without SPY features',
        'methodology': {
            'baseline': 'EXP-067 with 48 features (0.843 AUC)',
            'experimental': 'XGBoost with 41 features (no SPY-relative metrics)',
            'validation': '5-fold walk-forward time-series cross-validation',
            'success_criteria': 'AUC >= 0.70 AND Precision >= 0.25'
        }
    }

    results_file = os.path.join(results_dir, 'exp069_simplified_xgboost.json')
    with open(results_file, 'w') as f:
        json.dump(exp_results, f, indent=2, default=float)

    print(f"\nResults saved to: {results_file}")

    # Save model
    model_file = os.path.join(results_dir, 'exp069_simplified_model.json')
    results['model'].save_model(model_file)
    print(f"Model saved to: {model_file}")

    # Send email
    try:
        from src.notifications.sendgrid_notifier import SendGridNotifier
        notifier = SendGridNotifier()
        if notifier.is_enabled():
            print("\nSending simplified XGBoost report email...")
            notifier.send_experiment_report('EXP-069', exp_results)
        else:
            print("\n[INFO] Email not configured")
    except Exception as e:
        print(f"\n[WARNING] Email error: {e}")

    return exp_results


if __name__ == '__main__':
    """Run EXP-069: Simplified XGBoost for production deployment."""

    print("\n[ML OPTIMIZATION] Training production-compatible XGBoost")
    print("Removing SPY dependencies for immediate deployment")
    print()

    results = run_exp069_simplified_xgboost()

    print("\n" + "="*70)
    print("SIMPLIFIED XGBOOST TRAINING COMPLETE")
    print("="*70)
