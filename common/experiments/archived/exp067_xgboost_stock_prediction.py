"""
EXPERIMENT: EXP-067
Date: 2025-11-17
Objective: Train XGBoost model to predict profitable mean reversion signals

HYPOTHESIS:
Machine learning can identify stocks likely to experience profitable mean reversion
opportunities BEFORE they occur, enabling proactive position taking and better
portfolio allocation. Target: AUC > 0.60 for deployment.

ALGORITHM TESTED:
XGBoost Gradient Boosting Classifier
- Input: 55 technical features (returns, momentum, volatility, volume, trend, market)
- Target: Binary (will have profitable panic sell + recovery in next 7 days)
- Validation: Walk-forward time-series split (train on past, test on future)
- Metrics: AUC-ROC, Precision, Recall, Feature Importance

EXPECTED IMPACT:
- Improve signal quality by 15-25%
- Reduce false positives
- Enable portfolio expansion to 60+ stocks
- Foundation for fully automated ML-driven trading

SUCCESS CRITERIA:
- Out-of-sample AUC >= 0.60
- Precision >= 0.20 (worthwhile to trade on predictions)
- Feature importance reveals actionable insights
- Model generalizes across different market conditions
"""

import sys
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# ML imports
try:
    import xgboost as xgb
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import (
        roc_auc_score, precision_score, recall_score,
        classification_report, confusion_matrix
    )
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("[WARNING] ML libraries not installed")
    print("Install: pip install xgboost scikit-learn")


def load_feature_matrix(feature_file: str) -> pd.DataFrame:
    """Load the feature matrix generated by EXP-066."""
    try:
        df = pd.read_csv(feature_file)
        print(f"[OK] Loaded feature matrix: {len(df)} rows, {len(df.columns)} columns")
        return df
    except Exception as e:
        print(f"[ERROR] Failed to load feature matrix: {e}")
        return None


def prepare_ml_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, List[str]]:
    """
    Prepare data for ML training.

    Returns:
        X: Feature matrix
        y: Target variable
        feature_names: List of feature column names
    """
    # Columns to exclude from features
    exclude_cols = [
        'Date', 'ticker', 'Open', 'High', 'Low', 'Close', 'Volume',
        'target', 'forward_return_1d', 'forward_return_3d', 'forward_return_7d',
        'panic_sell', 'recovery_7d', 'rolling_max_20', 'drawdown'
    ]

    # Get feature columns
    feature_cols = [col for col in df.columns if col not in exclude_cols]

    # Handle missing values (forward fill then drop remaining)
    df_clean = df[feature_cols + ['target']].copy()
    df_clean = df_clean.ffill().bfill()
    df_clean = df_clean.dropna()

    X = df_clean[feature_cols]
    y = df_clean['target']

    print(f"[OK] Prepared ML data: {len(X)} samples, {len(feature_cols)} features")
    print(f"    Target distribution: {y.sum()} positive ({y.mean()*100:.1f}%), "
          f"{len(y) - y.sum()} negative ({(1-y.mean())*100:.1f}%)")

    return X, y, feature_cols


def train_xgboost_model(X: pd.DataFrame, y: pd.Series,
                       feature_names: List[str]) -> Dict:
    """
    Train XGBoost model with walk-forward validation.
    """
    print("\n" + "="*70)
    print("TRAINING XGBOOST MODEL")
    print("="*70)

    # Time-series split for walk-forward validation
    n_splits = 5
    tscv = TimeSeriesSplit(n_splits=n_splits)

    # Store results from each fold
    fold_results = []
    feature_importance_all = []

    print(f"\nUsing {n_splits}-fold time-series cross-validation...")
    print("-" * 70)

    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
        print(f"\nFold {fold}/{n_splits}")

        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        print(f"  Train: {len(X_train)} samples ({y_train.sum()} positive)")
        print(f"  Test:  {len(X_test)} samples ({y_test.sum()} positive)")

        # Calculate scale_pos_weight for class imbalance
        scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()

        # Train XGBoost
        model = xgb.XGBClassifier(
            objective='binary:logistic',
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=scale_pos_weight,
            random_state=42,
            n_jobs=-1
        )

        model.fit(X_train, y_train, verbose=False)

        # Predictions
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        y_pred = model.predict(X_test)

        # Metrics
        auc = roc_auc_score(y_test, y_pred_proba)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)

        print(f"  AUC: {auc:.3f} | Precision: {precision:.3f} | Recall: {recall:.3f}")

        fold_results.append({
            'fold': fold,
            'auc': float(auc),
            'precision': float(precision),
            'recall': float(recall),
            'test_samples': len(X_test),
            'test_positives': int(y_test.sum())
        })

        # Feature importance
        feature_importance_all.append(model.feature_importances_)

    # Aggregate results
    avg_auc = np.mean([r['auc'] for r in fold_results])
    avg_precision = np.mean([r['precision'] for r in fold_results])
    avg_recall = np.mean([r['recall'] for r in fold_results])

    print("\n" + "="*70)
    print("CROSS-VALIDATION RESULTS")
    print("="*70)
    print(f"Average AUC:       {avg_auc:.3f}")
    print(f"Average Precision: {avg_precision:.3f}")
    print(f"Average Recall:    {avg_recall:.3f}")

    # Feature importance (average across folds)
    avg_importance = np.mean(feature_importance_all, axis=0)
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': avg_importance
    }).sort_values('importance', ascending=False)

    print("\nTop 15 Most Important Features:")
    print("-" * 70)
    for idx, row in feature_importance_df.head(15).iterrows():
        print(f"  {row['feature']:30s} | {row['importance']:.4f}")

    # Train final model on all data
    print("\nTraining final model on full dataset...")
    scale_pos_weight_final = (len(y) - y.sum()) / y.sum()

    final_model = xgb.XGBClassifier(
        objective='binary:logistic',
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight_final,
        random_state=42,
        n_jobs=-1
    )

    final_model.fit(X, y, verbose=False)

    return {
        'avg_auc': avg_auc,
        'avg_precision': avg_precision,
        'avg_recall': avg_recall,
        'fold_results': fold_results,
        'feature_importance': feature_importance_df.to_dict('records'),
        'model': final_model
    }


def run_exp067_xgboost():
    """
    Train and evaluate XGBoost model for stock prediction.
    """
    print("="*70)
    print("EXP-067: XGBOOST STOCK PREDICTION MODEL")
    print("="*70)
    print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    print("OBJECTIVE: Predict which stocks will have profitable mean reversion signals")
    print("ALGORITHM: XGBoost Gradient Boosting Classifier")
    print("FEATURES: 55 technical indicators (momentum, volatility, volume, trend)")
    print("TARGET: Panic sell (>3% drop) + 7-day recovery (>3% gain)")
    print()

    if not ML_AVAILABLE:
        print("[ERROR] XGBoost/sklearn not available")
        print("Install: pip install xgboost scikit-learn")
        return None

    # Load feature matrix from EXP-066
    feature_file = 'logs/experiments/exp066_feature_matrix.csv'

    if not os.path.exists(feature_file):
        print(f"[ERROR] Feature matrix not found: {feature_file}")
        print("Run EXP-066 first to generate features")
        return None

    df = load_feature_matrix(feature_file)
    if df is None:
        return None

    print(f"\nDataset: {len(df)} total samples")
    print(f"Date range: {df['Date'].min()} to {df['Date'].max()}")
    print(f"Stocks: {df['ticker'].nunique()} unique tickers")
    print()

    # Prepare data
    X, y, feature_names = prepare_ml_data(df)

    # Train model
    results = train_xgboost_model(X, y, feature_names)

    # Deployment decision
    deploy = results['avg_auc'] >= 0.60 and results['avg_precision'] >= 0.20

    print("\n" + "="*70)
    print("DEPLOYMENT DECISION")
    print("="*70)
    print()

    if deploy:
        print(f"[SUCCESS] XGBoost model ready for deployment!")
        print()
        print(f"[OK] AUC: {results['avg_auc']:.3f} (>= 0.60 required)")
        print(f"[OK] Precision: {results['avg_precision']:.3f} (>= 0.20 required)")
        print()
        print("IMPACT:")
        print(f"  - Predicted positive class with {results['avg_precision']*100:.1f}% precision")
        print(f"  - Can identify {results['avg_recall']*100:.1f}% of profitable opportunities")
        print(f"  - ROC-AUC of {results['avg_auc']:.3f} indicates good predictive power")
        print()
        print("NEXT STEP: EXP-068 - Live testing on recent data")
        print("  - Test model predictions on last 3 months")
        print("  - Compare ML-selected vs random stock selection")
        print("  - Validate real-world performance")
    else:
        reasons = []
        if results['avg_auc'] < 0.60:
            reasons.append(f"AUC {results['avg_auc']:.3f} < 0.60 required")
        if results['avg_precision'] < 0.20:
            reasons.append(f"Precision {results['avg_precision']:.3f} < 0.20 required")

        print("[INSUFFICIENT PERFORMANCE]")
        for reason in reasons:
            print(f"  - {reason}")
        print()
        print("IMPROVEMENTS NEEDED:")
        print("  1. Add more features (fundamentals, sentiment)")
        print("  2. Try different algorithms (Random Forest, Neural Net)")
        print("  3. Tune hyperparameters (grid search)")
        print("  4. Balance dataset with SMOTE")

    # Save results
    results_dir = 'logs/experiments'
    os.makedirs(results_dir, exist_ok=True)

    exp_results = {
        'experiment_id': 'EXP-067',
        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'objective': 'Train XGBoost model to predict profitable mean reversion opportunities',
        'algorithm': 'XGBoost Gradient Boosting Classifier (5-fold time-series CV)',
        'dataset': {
            'samples': len(df),
            'features': len(feature_names),
            'date_range': f"{df['Date'].min()} to {df['Date'].max()}",
            'stocks': int(df['ticker'].nunique())
        },
        'performance': {
            'avg_auc': float(results['avg_auc']),
            'avg_precision': float(results['avg_precision']),
            'avg_recall': float(results['avg_recall'])
        },
        'fold_results': results['fold_results'],
        'top_features': results['feature_importance'][:20],
        'deploy': deploy,
        'next_step': 'EXP-068' if deploy else 'Model improvement',
        'hypothesis': 'ML can predict profitable mean reversion signals with AUC > 0.60',
        'methodology': {
            'baseline': 'Random stock selection (15.3% base rate)',
            'experimental': 'XGBoost predictions with 55 technical features',
            'validation': '5-fold walk-forward time-series cross-validation',
            'success_criteria': 'AUC >= 0.60 AND Precision >= 0.20'
        }
    }

    results_file = os.path.join(results_dir, 'exp067_xgboost_stock_prediction.json')
    with open(results_file, 'w') as f:
        json.dump(exp_results, f, indent=2, default=float)

    print(f"\nResults saved to: {results_file}")

    # Save model
    model_file = os.path.join(results_dir, 'exp067_xgboost_model.json')
    results['model'].save_model(model_file)
    print(f"Model saved to: {model_file}")

    # Send email
    try:
        from common.notifications.sendgrid_notifier import SendGridNotifier
        notifier = SendGridNotifier()
        if notifier.is_enabled():
            print("\nSending XGBoost model report email...")
            notifier.send_experiment_report('EXP-067', exp_results)
        else:
            print("\n[INFO] Email not configured")
    except Exception as e:
        print(f"\n[WARNING] Email error: {e}")

    return exp_results


if __name__ == '__main__':
    """Run EXP-067: XGBoost stock prediction model."""

    print("\n[ML PREDICTION] Training XGBoost to identify profitable opportunities")
    print("This enables proactive ML-driven stock selection")
    print()

    results = run_exp067_xgboost()

    print("\n" + "="*70)
    print("XGBOOST TRAINING COMPLETE")
    print("="*70)
